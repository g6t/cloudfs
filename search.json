[{"path":"https://g6t.github.io/cloudfs/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 cloudfs authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"project-assets","dir":"Articles","previous_headings":"Introduction","what":"Project assets","title":"cloudfs package overview","text":"terms digital assets data analysis project can usually divided three parts: input data: primarily tabular data (csv, sav, …), limited ; can e.g., raw text code sources: R Rmd scripts data processing happens outputs: spreadsheets, plots, presentations derived input data ’s common segregate storage code sources, input data, outputs. Code sources typically housed platforms like GitHub, optimized version control collaboration. Storing large input data platforms can inefficient. Systems like git can become overwhelmed tracking changes sizable data files. Hence, cloud storage solutions like Amazon S3 commonly chosen ability handle vast datasets. Outputs, results destined sharing review, usually stored separately code. Platforms like Google Drive preferred purpose. user-friendly interfaces easy sharing options ensure stakeholders can readily access review results.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"cloud-roots","dir":"Articles","previous_headings":"Introduction","what":"Cloud roots","title":"cloudfs package overview","text":"context, won’t differentiate assets inputs outputs. ’ll simply refer artifacts, setting apart code. package includes functions working S3 Google Drive. operations, use term cloud root denote primary folder either platform, holds project’s artifacts. Typically, root folder contains subfolders like “plots”, “data”, “results”, though exact structure isn’t crucial. project can root S3, Google Drive, platforms simultaneously.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"motivation","dir":"Articles","previous_headings":"Introduction","what":"Motivation","title":"cloudfs package overview","text":"Consider typical task: uploading file Amazon S3 using aws.s3 package illustrative example. Imagine ’re attempting upload R model saved RDS file located models/glm.rds. file destined project-1 directory within project-data bucket S3, representing dedicated S3 root project: Note following: Location Redundancy: Given project’s primary interactions “project-1” folder “project-data” bucket, ’re consistently faced specifying static location. Path Duplication: local system S3 use matching paths: models/glm.rds. uniformity typically practical making exceptions. Given repetitive nature code, ’s room streamlined approach. cloudfs package comes . set , uploading becomes much easier cleaner:","code":"aws.s3::put_object(   bucket = \"project-data\",   object = \"project-1/models/glm.rds\",   file = \"models/glm.rds\" ) cloud_s3_upload(\"models/glm.rds\")"},{"path":[]},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"setting-up-a-root","dir":"Articles","previous_headings":"Package walk-through","what":"Setting up a root","title":"cloudfs package overview","text":"begin working cloudfs package R project, first set cloud root. S3 use cloud_s3_attach(), Google Drive, use cloud_drive_attach() function. Let’s set Google Drive root: Upon execution, ’ll prompted input URL intended Google Drive folder serve project’s root. location registered project’s DESCRIPTION file. conveniently access directory future, execute cloud_drive_browse().","code":"library(cloudfs) cloud_drive_attach()"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"types-of-interactions","dir":"Articles","previous_headings":"Package walk-through","what":"Types of interactions","title":"cloudfs package overview","text":"Now let’s talk actual interactions cloud storage. Data transfer actions can categorized two parameters: direction – whether ’re uploading data cloud retrieving data . file R object – using cloudfs, can upload download files cloud storages also directly read write objects cloud. cloudfs functions moving files use “upload” “download” names. Functions direct reading writing use “read” “write”. S3-specific functions contain “s3”, Google Drive ones use “drive”.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"practical-examples","dir":"Articles","previous_headings":"Package walk-through","what":"Practical Examples","title":"cloudfs package overview","text":", ’ll demonstrate hands-application cloudfs functions data transfer. Upon successfully completing cloud_drive_attach() process, project associated designated Google Drive root. initial step, create save ggplot scatterplot local PNG file purpose demonstration. upload file Google Drive, execute: invoking cloud_drive_ls() function, can view automatically created “plots” folder console. inspect contents folder, currently contains single PNG file, use cloud_drive_ls(\"plots\") cloud_drive_ls(recursive = TRUE). access folder Google Drive, execute cloud_drive_browse(\"plots\"). directly view scatterplot, use cloud_drive_browse(\"plots/scatterplot.png\"). cloudfs, can directly write content cloud storage, bypassing manual creation local files. file generation process remains transparent user. First, let’s compute summary mtcars dataframe: export summary spreadsheet, simply specify desired file path appropriate extension. method writing inferred extension: view resulting spreadsheet Google Drive, execute cloud_drive_browse(\"results/mtcars_summary.xlsx\"). Just wrote summary xlsx file, can also read using cloud_drive_read(\"results/mtcars_summary.xlsx\"). ’s noteworthy writing reading methods determined automatically based file extension. instance, “.xlsx” utilizes writexl::write_xlsx() reading, whereas “.csv” employs readr::write_csv. comprehensive list default methods available documentation cloud_drive_write() cloud_drive_read() functions. Additionally, cloudfs offers flexibility allowing custom writing reading methods. instance, earlier scatterplot written directly Google Drive, bypassing local file generation:","code":"library(ggplot2) p <- ggplot(mtcars, aes(mpg, disp)) + geom_point() if (!dir.exists(\"plots\")) dir.create(\"plots\") ggsave(plot = p, filename = \"plots/scatterplot.png\") cloud_drive_upload(\"plots/scatterplot.png\") library(dplyr, quietly = TRUE) summary_df <-    mtcars %>%    group_by(cyl) %>%    summarise(across(disp, mean)) cloud_drive_write(summary_df, \"results/mtcars_summary.xlsx\") cloud_drive_write(   p, \"plots/scatterplot.png\",   fun = \\(x, file)      ggsave(plot = x, filename = file) )"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"operations-on-multiple-files-at-once","dir":"Articles","previous_headings":"Package walk-through","what":"Operations on multiple files at once","title":"cloudfs package overview","text":"Suppose multiple CSV files uploaded “data” folder intend download locally. Instead invoking cloud_s3_download() file, efficient approach available. first, let’s generate sample files demonstration purposes. Listing contents “data” folder gives us following: cloudfs offers bulk functions simplify management multiple files simultaneously. instance, download files listed use cloud_drive_download_bulk(): action automatically downloads datasets local “data” directory, replicating structure Google Drive. read several CSV files “data” folder Google Drive consolidated list, execute: upload collection objects, ggplot visualizations, Google Drive, first group named list. , utilize cloud_object_ls() function generate dataframe akin output cloud_drive_ls(). Finally, execute cloud_drive_write_bulk() complete upload. bulk uploads local files Google Drive, utilize cloud_local_ls() function. instance, upload PNG files local “plots” directory Google Drive:","code":"cloud_drive_write(datasets::airquality, \"data/airquality.csv\") cloud_drive_write(datasets::trees, \"data/trees.csv\") cloud_drive_write(datasets::beaver1, \"data/beaver1.csv\") cloud_drive_ls(\"data\") #> # A tibble: 3 × 5 #>   name           type  last_modified       size_b id       #>   <chr>          <chr> <dttm>               <dbl> <drv_id> #> 1 airquality.csv csv   2023-09-12 08:04:46   2890 1CXTi1A… #> 2 beaver1.csv    csv   2023-09-12 08:04:50   1901 1Fg4s1O… #> 3 trees.csv      csv   2023-09-12 08:04:48    400 1vDYBVt… cloud_drive_ls(\"data\") %>%    cloud_drive_download_bulk() all_data <-    cloud_drive_ls(\"data\") %>%    cloud_drive_read_bulk() library(ggplot2) p1 <- ggplot(mtcars, aes(mpg, disp)) + geom_point() p2 <- ggplot(mtcars, aes(cyl)) + geom_bar()  plots_list <-    list(\"plot_1\" = p1, \"plot_2\" = p2) %>%    cloud_object_ls(path = \"plots\", extension = \"png\", suffix = \"_newsletter\")  plots_list %>%    cloud_drive_write_bulk(fun = \\(x, file) ggsave(plot = x, filename = file)) cloud_local_ls(\"plots\") %>%    filter(type == \"png\") %>%    cloud_drive_upload_bulk()"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"s3-functions","dir":"Articles","previous_headings":"Package walk-through","what":"S3 functions","title":"cloudfs package overview","text":"Amazon S3 interactions, offer parallel set functions similar designed Google Drive. dedicated S3 functions easily identifiable, beginning prefix cloud_s3_.","code":""},{"path":"https://g6t.github.io/cloudfs/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Iaroslav Domin. Author, maintainer. Stefan Musch. Author. Michal Czyz. Author. Emmanuel Ugochukwu. Author. . Copyright holder, funder.","code":""},{"path":"https://g6t.github.io/cloudfs/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Domin , Musch S, Czyz M, Ugochukwu E (2023). cloudfs: Streamlined Interface Interact Cloud Storage Platforms. https://g6t.github.io/cloudfs/, https://github.com/g6t/cloudfs.","code":"@Manual{,   title = {cloudfs: Streamlined Interface to Interact with Cloud Storage Platforms},   author = {Iaroslav Domin and Stefan Musch and Michal Czyz and Emmanuel Ugochukwu},   year = {2023},   note = {https://g6t.github.io/cloudfs/, https://github.com/g6t/cloudfs}, }"},{"path":"https://g6t.github.io/cloudfs/index.html","id":"cloudfs-","dir":"","previous_headings":"","what":"Streamlined Interface to Interact with Cloud Storage Platforms","title":"Streamlined Interface to Interact with Cloud Storage Platforms","text":"cloudfs R package developed Gradient Metrics offers unified interface simplifying cloud storage interactions. cloudfs supports uploading, downloading, reading, writing files Google Drive Amazon S3.","code":""},{"path":"https://g6t.github.io/cloudfs/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Streamlined Interface to Interact with Cloud Storage Platforms","text":"","code":"# from CRAN install.packages(\"cloudfs\")  # from GitHub remotes::install_github(\"g6t/cloudfs\")"},{"path":"https://g6t.github.io/cloudfs/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Streamlined Interface to Interact with Cloud Storage Platforms","text":"Relative path simplicity Use paths relative project’s main cloud folder. Unified interface Google Drive S3 Downloading S3? process just straightforward. Extension-aware functions package automatically selects right read write function based file extension, simplifying interactions. don’t like default function, can use different one explicitly calling . Effortless cloud navigation Open folders browser list contents console. Bulk file management Easily retrieve data folder one go push multiple files cloud .","code":"cloud_drive_download(\"raw_data/transactions.xlsx\") cloud_s3_download(\"raw_data/transactions.xlsx\") cloud_s3_write(glmnet_model, \"models/glmnet.rds\") cloud_s3_write(glmnet_model, \"models/glmnet.rds\", fun = readr::write_rds) cloud_drive_browse(\"plots\") cloud_s3_ls(\"data\") all_data <-    cloud_s3_ls(\"data\") %>%   cloud_s3_read_bulk() cloud_local_ls(\"plots\") %>% cloud_drive_upload_bulk()"},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":null,"dir":"Reference","previous_headings":"","what":"Capture Arguments — check_args","title":"Capture Arguments — check_args","text":"Helper catch arguments.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Capture Arguments — check_args","text":"","code":"check_args(...)"},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Capture Arguments — check_args","text":"... unquoted arguments names","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Capture Arguments — check_args","text":"List quosures.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Argument is Single TRUE or FALSE — check_bool","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"Check argument single TRUE FALSE. option possible allow NULL value alt_null = TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"","code":"check_bool(x, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"x Function argument asserted. alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_name} message body (e.g. \"{x_name}\"); pass class arguments failed test using {wrong_class} message body (e.g. \"{wrong_class} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"argument single TRUE FALSE (optionally NULL) returns invisible NULL. Otherwise function throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Argument's Class — check_class","title":"Check Argument's Class — check_class","text":"Check argument proper class.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Argument's Class — check_class","text":"","code":"check_class(x, arg_class, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Argument's Class — check_class","text":"x Function argument asserted. arg_class Class name. Usually \"character\", \"numeric\", \"data.frame\", etc. alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_names} message body (e.g. \"{x_names}\"); pass tested class using {arg_class} message body (e.g. \"want {arg_class})\" pass classes arguments failed test using {wrong_class} message body (e.g. \"{wrong_class} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Argument's Class — check_class","text":"argument class arg_class returns invisible NULL. Otherwise function throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Argument is of Proper Length — check_length","title":"Check if Argument is of Proper Length — check_length","text":"TODO.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Argument is of Proper Length — check_length","text":"","code":"check_length(x, arg_length = 1L, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Argument is of Proper Length — check_length","text":"x Function arguments asserted. arg_length Integer. Length argument, scalars take value 1 (default). alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_name} message body (e.g. \"{wrong_names}\"); pass tested length using {arg_length} message body (e.g. \"want {arg_length})\" pass lengths arguments failed test using {wrong_length} message body (e.g. \"{wrong_lengths} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Argument is of Proper Length — check_length","text":"Returns invisible NULL argument asserted length, otherwise throw error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":null,"dir":"Reference","previous_headings":"","what":"Return check_null Value — check_null_cond","title":"Return check_null Value — check_null_cond","text":"Check alt_null argument TRUE FALSE. FALSE return FALSE. argument TRUE check x argument NULL return logical value.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return check_null Value — check_null_cond","text":"","code":"check_null_cond(x, alt_null)"},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return check_null Value — check_null_cond","text":"x Argument check NULL. alt_null Logical. TRUE check x NULL.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return check_null Value — check_null_cond","text":"Either TRUE FALSE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Function Argument is Scalar — check_scalar","title":"Check if Function Argument is Scalar — check_scalar","text":"function check argument proper class length 1.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Function Argument is Scalar — check_scalar","text":"","code":"check_scalar(..., arg_class, alt_null = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Function Argument is Scalar — check_scalar","text":"... Function argument asserted. arg_class Class name. Usually \"character\", \"numeric\", \"data.frame\", etc. alt_null Logical. argument accept NULL value.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Function Argument is Scalar — check_scalar","text":"Invisible NULL assertion TRUE, otherwise error message.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":null,"dir":"Reference","previous_headings":"","what":"User Interface: Ask a Yes/No question — cli_yeah","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"function inspired (mostly copied ) usethis::ui_yeah function. purpose ask user yes/question. differences : limited answer options customization. done purpose standardize command line dialogues code. uses cli package hood, cli rich text formatting possible.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"","code":"cli_yeah(x, straight = FALSE, .envir = parent.frame())"},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"x Question display. straight (logical) Ask straight Yes/question? default (FALSE), two different \"\" options one \"yes\" option sampled pool variants. words behaves just like usethis::ui_yeah default parameter setup. straight = TRUE, shows \"Yes\" \"\", literally. .envir Environment evaluate glue expressions .","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"(logical) Returns TRUE user selects \"yes\" option FALSE otherwise, .e. user selects \"\" option refuses make selection (cancels).","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":null,"dir":"Reference","previous_headings":"","what":"Attach Google Drive folder to project — cloud_drive_attach","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"function facilitates association specific Google Drive folder project adding unique identifier project's DESCRIPTION file. user prompted navigate Google Drive website, select create desired folder project, provide URL. function extracts necessary information URL updates cloudfs.drive field DESCRIPTION file accordingly.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"","code":"cloud_drive_attach(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"function return meaningful value. primary purpose side effect updating project's DESCRIPTION file associated Google Drive folder identifier.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"","code":"if (FALSE) { # interactive() cloud_drive_attach() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse project's Google Drive folder — cloud_drive_browse","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"Opens project's Google Drive folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"","code":"cloud_drive_browse(path = \"\", root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"path (optional) Path inside Google Drive folder open. Defaults root level (path = \"\") project's folder. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"Invisibly returns NULL. primary purpose function side effect: opening specified Google Drive folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"","code":"if (FALSE) { # interactive() cloud_drive_browse() cloud_drive_browse(\"models/kmeans\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a file from Google Drive to the local project folder — cloud_drive_download","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"Retrieves file project's Google Drive folder saves local project folder, maintaining original folder structure.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"","code":"cloud_drive_download(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"Invisibly returns NULL successfully downloading file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a file from Google Drive to the local project folder — cloud_drive_download","text":"","code":"if (FALSE) { # interactive() # downloads toy_data/demo.csv from project's Google Drive folder  # (provided it exists) and saves it to local 'toy_data' folder cloud_drive_download(\"toy_data/demo.csv\")  # clean up unlink(\"toy_data\", recursive = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk download contents from Google Drive — cloud_drive_download_bulk","title":"Bulk download contents from Google Drive — cloud_drive_download_bulk","text":"Downloads multiple files Google Drive folder based output dataframe cloud_drive_ls. function streamlines process downloading multiple files allowing filter select specific files Google Drive listing download bulk.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk download contents from Google Drive — cloud_drive_download_bulk","text":"","code":"cloud_drive_download_bulk(content, quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk download contents from Google Drive — cloud_drive_download_bulk","text":"content (data.frame) Output cloud_drive_ls() quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk download contents from Google Drive — cloud_drive_download_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk download contents from Google Drive — cloud_drive_download_bulk","text":"","code":"if (FALSE) { # interactive() # provided there's a folder called \"toy_data\" in the root of your project's # Google Drive folder, and this folder contains \"csv\" files cloud_drive_ls(\"toy_data\") |>    filter(type == \"csv\") |>    cloud_drive_download_bulk()    # clean up unlink(\"toy_data\", recursive = TRUE)    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Google Drive folder based on a path — cloud_drive_find_path","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"Given Google Drive id pointing folder relative path inside folder, returns id object (file folder) corresponding path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"","code":"cloud_drive_find_path(root, path = \"\", create = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"root ID folder start search . path Relative location respect root folder. create Create folders describing path exist? Default FALSE default function throws error path found. TRUE, function create missing subdirectories. Note object deepest level always created folder. E.g. path = \"models/kmeans/model.Rds\" \"model.Rds\" missing, function create folder name.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"googledrive::dribble object corresponding folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of Project's Google Drive Folder — cloud_drive_ls","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"Returns tibble names, timestamps, sizes files folders inside specified Google Drive folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"","code":"cloud_drive_ls(path = \"\", recursive = FALSE, full_names = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"path (optional) Path inside Google Drive root folder. Specifies subfolder whose contents listed. default, path = \"\", lists root-level files folders. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"tibble containing names, last modification timestamps, sizes bytes, Google Drive IDs files folders inside specified Google Drive folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"","code":"if (FALSE) { # interactive() # list only root-level files and folders cloud_drive_ls()   # list all files in all nested folders cloud_drive_ls(recursive = TRUE)  # list contents of \"plots/barplots\" subfolder cloud_drive_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"cloud_drive_ls returns dataframe contents Google Drive folder. dataframe name, type id columns. name may either full names short names (depending full_names parameter cloud_drive_ls), names(name) always contain full names. function: filters folders extracts names(name) path column. informs size files downloaded/read asks confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"","code":"cloud_drive_prep_bulk(   content,   what = c(\"read\", \"download\"),   safe_size = 5e+07,   quiet = FALSE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"content (data.frame) Output cloud_drive_ls() done content, either \"read\" \"download\". affects messages look. safe_size considered safe size bytes download bulk. show additional caution message case accidentally run bulk reading folder gigabytes data. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"Transformed content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a file from Google Drive — cloud_drive_read","title":"Read a file from Google Drive — cloud_drive_read","text":"Retrieves reads file project's Google Drive folder. default, function attempts determine appropriate reading function based file's extension. However, can specify custom reading function necessary.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a file from Google Drive — cloud_drive_read","text":"","code":"cloud_drive_read(file, fun = NULL, ..., root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a file from Google Drive — cloud_drive_read","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun custom reading function. NULL (default), appropriate reading function inferred based file's extension. ... Additional arguments pass reading function fun. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a file from Google Drive — cloud_drive_read","text":"content file read Google Drive, additional attributes containing metadata file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Read a file from Google Drive — cloud_drive_read","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a file from Google Drive — cloud_drive_read","text":"","code":"if (FALSE) { # interactive() # provided there are folders called \"data\" and \"models\" in the root of your # project's main Google Drive folder and they contain the files mentioned # below cloud_drive_read(\"data/mtcars.csv\") cloud_drive_read(\"models/random_forest.rds\") cloud_drive_read(\"data/dm.sas7bdat\", fun = haven::read_sas) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","title":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","text":"function facilitates bulk reading multiple files project's designated Google Drive folder. using cloud_drive_ls, can obtain dataframe detailing contents Google Drive folder. Applying cloud_drive_read_bulk dataframe allows read listed files named list. function , default, infer appropriate reading method based file's extension. However, specific reading function provided via fun parameter, applied uniformly files, may suitable diverse file types.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","text":"","code":"cloud_drive_read_bulk(content, fun = NULL, ..., quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","text":"content (data.frame) Output cloud_drive_ls() fun custom reading function. NULL (default), appropriate reading function inferred based file's extension. ... Additional arguments pass reading function fun. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","text":"named list element corresponds content file Google Drive. names list elements derived file names.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk Read Contents from Google Drive — cloud_drive_read_bulk","text":"","code":"if (FALSE) { # interactive() # provided there's a folder called \"data\" in the root of the project's main # Google Drive folder, and it contains csv files data_lst <-    cloud_drive_ls(\"data\") |>    filter(type == \"csv\") |>    cloud_drive_read_bulk()    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"Finds spreadsheet path relative project root. Applies googlesheets4::range_autofit() sheet.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"","code":"cloud_drive_spreadsheet_autofit(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"file ID resized Google spreadsheet invisible result.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"","code":"if (FALSE) { # interactive() cloud_drive_write(mtcars, \"results/mtcars.xlsx\") cloud_drive_spreadsheet_autofit(\"results/mtcars.xlsx\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a local file to Google Drive — cloud_drive_upload","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"Uploads local file project's directory corresponding location within project's Google Drive root folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"","code":"cloud_drive_upload(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"Invisibly returns googledrive::dribble object representing uploaded file Google Drive.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"","code":"if (FALSE) { # interactive() # create a toy csv file dir.create(\"toy_data\") write.csv(mtcars, \"toy_data/mtcars.csv\")  # uploads toy_data/mtcars.csv to 'data' subfolder of project's  # Google Drive folder cloud_drive_upload(\"toy_data/mtcars.csv\")  # clean up unlink(\"toy_data\", recursive = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","title":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","text":"function streamlines process uploading multiple files local project folder project's designated Google Drive folder. using cloud_local_ls, can obtain dataframe detailing contents local folder. Applying cloud_drive_upload_bulk dataframe allows upload listed files Google Drive.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","text":"","code":"cloud_drive_upload_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk Upload Files to Google Drive — cloud_drive_upload_bulk","text":"","code":"if (FALSE) { # interactive() # create toy plots: 2 png's and 1 jpeg dir.create(\"toy_plots\") png(\"toy_plots/plot1.png\"); plot(rnorm(100)); dev.off() png(\"toy_plots/plot2.png\"); plot(hist(rnorm(100))); dev.off() png(\"toy_plots/plot3.jpeg\"); plot(hclust(dist(USArrests), \"ave\")); dev.off()  # upload only the two png's cloud_local_ls(\"toy_plots\")  |>    dplyr::filter(type == \"png\")  |>    cloud_drive_upload_bulk()  # clean up unlink(\"toy_plots\", recursive = TRUE)    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an object to Google Drive — cloud_drive_write","title":"Write an object to Google Drive — cloud_drive_write","text":"Saves R object designated location project's Google Drive folder. custom writing function provided, function infer appropriate writing method based file's extension.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an object to Google Drive — cloud_drive_write","text":"","code":"cloud_drive_write(x, file, fun = NULL, ..., local = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an object to Google Drive — cloud_drive_write","text":"x R object written Google Drive. file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun custom writing function. NULL (default), appropriate writing function inferred based file's extension. ... Additional arguments pass writing function fun. local Logical. TRUE, local copy file also created specified path. Default FALSE. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write an object to Google Drive — cloud_drive_write","text":"Invisibly returns googledrive::dribble object representing written file Google Drive.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Write an object to Google Drive — cloud_drive_write","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an object to Google Drive — cloud_drive_write","text":"","code":"if (FALSE) { # interactive() # write mtcars dataframe to mtcars.csv in data folder cloud_drive_write(mtcars, \"data/mtcars.csv\") cloud_drive_write(random_forest, \"models/random_forest.rds\")  # provide custom writing function with parameters  cloud_drive_write(c(\"one\", \"two\"), \"text/count.txt\", writeLines, sep = \"\\n\\n\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","title":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","text":"function allows bulk writing multiple R objects project's designated Google Drive folder. prepare list objects writing, use cloud_object_ls, generates dataframe listing objects intended destinations format akin output cloud_drive_ls. default, function determines appropriate writing method based file's extension. However, specific writing function provided via fun parameter, applied files, may ideal dealing variety file types.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","text":"","code":"cloud_drive_write_bulk(   content,   fun = NULL,   ...,   local = FALSE,   quiet = FALSE,   root = NULL )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","text":"content (data.frame) output cloud_object_ls() fun custom writing function. NULL (default), appropriate writing function inferred based file's extension. ... Additional arguments pass writing function fun. local Logical. TRUE, local copy file also created specified path. Default FALSE. quiet caution messages may turned setting parameter TRUE. root Google Drive ID URL project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write multiple objects to Google Drive in bulk — cloud_drive_write_bulk","text":"","code":"if (FALSE) { # interactive() # write two csv files: data/df_mtcars.csv and data/df_iris.csv cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) |>  cloud_drive_write_bulk()    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cloud roots of a project — cloud_get_roots","title":"Get cloud roots of a project — cloud_get_roots","text":"Returns list cloudfs.* roots defined project's DESCRIPTION.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cloud roots of a project — cloud_get_roots","text":"","code":"cloud_get_roots(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cloud roots of a project — cloud_get_roots","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get cloud roots of a project — cloud_get_roots","text":"named list element corresponds cloudfs.* root defined project's DESCRIPTION file. names list elements derived cloudfs.* fields removing cloudfs. prefix.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get cloud roots of a project — cloud_get_roots","text":"","code":"# create a temp. folder, and put DESCRIPTION file with cloudfs.* fields into it tmp_project <- file.path(tempdir(), \"cloudfs\") if (!dir.exists(tmp_project)) dir.create(tmp_project) tmp_project_desc <- file.path(tmp_project, \"DESCRIPTION\") desc_content <- c(   \"Package: -\",   \"cloudfs.s3: my_bucket/my_project\",   \"cloudfs.drive: aaaaaa\" ) writeLines(desc_content, tmp_project_desc)  roots <- cloud_get_roots(tmp_project) roots #> $s3 #> [1] \"my_bucket/my_project\" #>  #> $drive #> [1] \"aaaaaa\" #>"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Guess reading function based on file extensions — cloud_guess_read_fun","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"Take look switch call. basically . Returns appropriate function throws error able find one.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"","code":"cloud_guess_read_fun(file)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"reading function.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Guess writing function based on file extensions — cloud_guess_write_fun","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"Take look switch call. basically . Returns appropriate function throws error able find one.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"","code":"cloud_guess_write_fun(file)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"writing function.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of local project folder — cloud_local_ls","title":"List Contents of local project folder — cloud_local_ls","text":"Retrieves names, timestamps, sizes files folders inside local project folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of local project folder — cloud_local_ls","text":"","code":"cloud_local_ls(   path = \"\",   root = \".\",   recursive = FALSE,   full_names = FALSE,   ignore = TRUE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of local project folder — cloud_local_ls","text":"path (optional) Path, relative specified root list contents . default, path = \"\", lists root-level files folders. root Local directory path relative paths considered. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. ignore Logical flag indicating whether ignore certain directories. Currently, set TRUE, 'renv' folder ignored due typically large size. parameter may expanded future support complex ignore patterns.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Contents of local project folder — cloud_local_ls","text":"tibble containing names, last modification timestamps, sizes bytes files folders inside specified local folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of local project folder — cloud_local_ls","text":"","code":"# list only root-level files and folders cloud_local_ls()  #> # A tibble: 27 × 4 #>    name                    type  last_modified       size_b #>    <chr>                   <chr> <dttm>               <int> #>  1 Rplot001.png            png   2023-10-18 18:19:07   1011 #>  2 check_args.html         html  2023-10-18 18:18:58   7795 #>  3 check_bool.html         html  2023-10-18 18:18:58   9000 #>  4 check_class.html        html  2023-10-18 18:18:58   8916 #>  5 check_length.html       html  2023-10-18 18:18:59   8899 #>  6 check_null_cond.html    html  2023-10-18 18:18:59   8518 #>  7 check_scalar.html       html  2023-10-18 18:18:59   8360 #>  8 cli_yeah.html           html  2023-10-18 18:18:59   9791 #>  9 cloud_drive_attach.html html  2023-10-18 18:19:00  10017 #> 10 cloud_drive_browse.html html  2023-10-18 18:19:00  10185 #> # ℹ 17 more rows  # list all files in all nested folders cloud_local_ls(recursive = TRUE) #> # A tibble: 29 × 4 #>    name                    type  last_modified       size_b #>    <chr>                   <chr> <dttm>               <int> #>  1 Rplot001.png            png   2023-10-18 18:19:07   1011 #>  2 check_args.html         html  2023-10-18 18:18:58   7795 #>  3 check_bool.html         html  2023-10-18 18:18:58   9000 #>  4 check_class.html        html  2023-10-18 18:18:58   8916 #>  5 check_length.html       html  2023-10-18 18:18:59   8899 #>  6 check_null_cond.html    html  2023-10-18 18:18:59   8518 #>  7 check_scalar.html       html  2023-10-18 18:18:59   8360 #>  8 cli_yeah.html           html  2023-10-18 18:18:59   9791 #>  9 cloud_drive_attach.html html  2023-10-18 18:19:00  10017 #> 10 cloud_drive_browse.html html  2023-10-18 18:19:00  10185 #> # ℹ 19 more rows  if (FALSE) { # list contents of \"plots/barplots\" subfolder (if it exists) cloud_local_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","title":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","text":"cloud_*_ls functions cloud locations (e.g. cloud_s3_ls) return content dataframes can passed cloud_*_read_bulk cloud_*_download_bulk functions read/download multiple files . similar manner, function accepts list objects input produces dataframe can passed cloud_*_write_bulk functions write multiple files .","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","text":"","code":"cloud_object_ls(x, path, extension, prefix = \"\", suffix = \"\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","text":"x named list. Names may contain letters, digits, spaces, '.', '-', '_' symbols contain trailing leading spaces. path directory relative project root write objects . extension File extension (string) without leading dot. prefix, suffix (optional) strings attach beginning end file names.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","text":"tibble row represents object input list, comprising following columns: object - objects provided name - contains paths objects meant written.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a dataframe for bulk writing of objects to cloud — cloud_object_ls","text":"","code":"cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) #> # A tibble: 2 × 3 #>   object         name               type  #>   <named list>   <chr>              <chr> #> 1 <df [32 × 11]> data/df_mtcars.csv csv   #> 2 <df [150 × 5]> data/df_iris.csv   csv"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"cloud_object_ls returns ls-like dataframe named list objects. function used mainly inform user files going written ask confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"","code":"cloud_object_prep_bulk(content, quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"content (data.frame) output cloud_object_ls() quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"Modified content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare ls output — cloud_prep_ls","title":"Prepare ls output — cloud_prep_ls","text":"hood ls functions (s3, drive, local) obtain information folder content recursively regardless recursive parameter. needed able calculate last modified time size folders case recursive set FALSE. content presented form dataframe similar see run ls function recursive = TRUE full_names = FALSE. function takes dataframe point : Summarizes non-recursive output recursive FALSE. Appends path names full_names TRUE. Writes full names names name column regardless full_names parameter. Evaluates type column.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare ls output — cloud_prep_ls","text":"","code":"cloud_prep_ls(data, path, recursive, full_names)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare ls output — cloud_prep_ls","text":"data ls dataframe assembled internally cloud_ls_* function path path used cloud_ls_* function recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare ls output — cloud_prep_ls","text":"Transformed data.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":null,"dir":"Reference","previous_headings":"","what":"Read excel file as a list of dataframes — cloud_read_excel","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"Uses readxl::read_excel hood, reads sheets returns named list dataframes.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"","code":"cloud_read_excel(path)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"path Path xlsx/xls file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"named list dataframes, dataframe corresponds sheet Excel file. names list elements derived sheet names.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"","code":"datasets <- readxl::readxl_example(\"datasets.xlsx\") cloud_read_excel(datasets) #> $iris #> # A tibble: 150 × 5 #>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species #>           <dbl>       <dbl>        <dbl>       <dbl> <chr>   #>  1          5.1         3.5          1.4         0.2 setosa  #>  2          4.9         3            1.4         0.2 setosa  #>  3          4.7         3.2          1.3         0.2 setosa  #>  4          4.6         3.1          1.5         0.2 setosa  #>  5          5           3.6          1.4         0.2 setosa  #>  6          5.4         3.9          1.7         0.4 setosa  #>  7          4.6         3.4          1.4         0.3 setosa  #>  8          5           3.4          1.5         0.2 setosa  #>  9          4.4         2.9          1.4         0.2 setosa  #> 10          4.9         3.1          1.5         0.1 setosa  #> # ℹ 140 more rows #>  #> $mtcars #> # A tibble: 32 × 11 #>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb #>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4 #>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4 #>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1 #>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1 #>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2 #>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1 #>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4 #>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2 #>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2 #> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4 #> # ℹ 22 more rows #>  #> $chickwts #> # A tibble: 71 × 2 #>    weight feed      #>     <dbl> <chr>     #>  1    179 horsebean #>  2    160 horsebean #>  3    136 horsebean #>  4    227 horsebean #>  5    217 horsebean #>  6    168 horsebean #>  7    108 horsebean #>  8    124 horsebean #>  9    143 horsebean #> 10    140 horsebean #> # ℹ 61 more rows #>  #> $quakes #> # A tibble: 1,000 × 5 #>      lat  long depth   mag stations #>    <dbl> <dbl> <dbl> <dbl>    <dbl> #>  1 -20.4  182.   562   4.8       41 #>  2 -20.6  181.   650   4.2       15 #>  3 -26    184.    42   5.4       43 #>  4 -18.0  182.   626   4.1       19 #>  5 -20.4  182.   649   4         11 #>  6 -19.7  184.   195   4         12 #>  7 -11.7  166.    82   4.8       43 #>  8 -28.1  182.   194   4.4       15 #>  9 -28.7  182.   211   4.7       35 #> 10 -17.5  180.   622   4.3       19 #> # ℹ 990 more rows #>"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":null,"dir":"Reference","previous_headings":"","what":"Attach S3 folder to project — cloud_s3_attach","title":"Attach S3 folder to project — cloud_s3_attach","text":"function facilitates association specific S3 folder project adding unique identifier project's DESCRIPTION file. user prompted navigate S3 console, select create desired folder project, provide URL. function extracts necessary information URL updates cloudfs.s3 field DESCRIPTION file accordingly.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Attach S3 folder to project — cloud_s3_attach","text":"","code":"cloud_s3_attach(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Attach S3 folder to project — cloud_s3_attach","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Attach S3 folder to project — cloud_s3_attach","text":"function return meaningful value modifies DESCRIPTION file specified project include S3 folder path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Attach S3 folder to project — cloud_s3_attach","text":"","code":"if (FALSE) { # interactive() cloud_s3_attach() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse project's S3 folder — cloud_s3_browse","title":"Browse project's S3 folder — cloud_s3_browse","text":"Opens project's S3 folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse project's S3 folder — cloud_s3_browse","text":"","code":"cloud_s3_browse(path = \"\", root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse project's S3 folder — cloud_s3_browse","text":"path (optional) Path inside S3 folder open. Defaults root level (path = \"\") project's S3 folder. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Browse project's S3 folder — cloud_s3_browse","text":"Invisibly returns NULL. primary purpose function side effect: opening specified S3 folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse project's S3 folder — cloud_s3_browse","text":"","code":"if (FALSE) { # interactive() cloud_s3_browse() cloud_s3_browse(\"data\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a file from S3 to the local project folder — cloud_s3_download","title":"Download a file from S3 to the local project folder — cloud_s3_download","text":"Retrieves file project's S3 root folder saves local project folder, maintaining original folder structure.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a file from S3 to the local project folder — cloud_s3_download","text":"","code":"cloud_s3_download(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a file from S3 to the local project folder — cloud_s3_download","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download a file from S3 to the local project folder — cloud_s3_download","text":"Invisibly returns NULL successfully downloading file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a file from S3 to the local project folder — cloud_s3_download","text":"","code":"if (FALSE) { # interactive() # downloads toy_data/demo.csv from project's S3 folder (provided it exists) # and saves it to local 'toy_data' folder cloud_s3_download(\"toy_data/demo.csv\")  # clean up unlink(\"toy_data\", recursive = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Download Contents from S3 — cloud_s3_download_bulk","title":"Bulk Download Contents from S3 — cloud_s3_download_bulk","text":"Downloads multiple files S3 folder based output dataframe cloud_s3_ls. function streamlines process downloading multiple files allowing filter select specific files S3 listing download bulk.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Download Contents from S3 — cloud_s3_download_bulk","text":"","code":"cloud_s3_download_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Download Contents from S3 — cloud_s3_download_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Download Contents from S3 — cloud_s3_download_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk Download Contents from S3 — cloud_s3_download_bulk","text":"","code":"if (FALSE) { # interactive() # provided there's a folder called \"toy_data\" in the root of your project's # S3 folder, and this folder contains \"csv\" files cloud_s3_ls(\"toy_data\") |>    filter(type == \"csv\") |>    cloud_s3_download_bulk()    # clean up unlink(\"toy_data\", recursive = TRUE)    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of Project's S3 Folder — cloud_s3_ls","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"Returns tibble names, timestamps, sizes files folders inside specified S3 folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"","code":"cloud_s3_ls(path = \"\", recursive = FALSE, full_names = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"path (optional) Path inside S3 folder. Specifies subfolder whose contents listed. default, path = \"\", lists root-level files folders. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"tibble containing names, last modification timestamps, sizes bytes files folders inside specified S3 folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"","code":"if (FALSE) { # interactive() # list only root-level files and folders cloud_s3_ls()   # list all files in all nested folders cloud_s3_ls(recursive = TRUE)  # list contents of \"plots/barplots\" subfolder cloud_s3_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"cloud_s3_ls returns dataframe contents S3 folder. dataframe name type columns. name may either full names short names (depending full_names parameter cloud_s3_ls), names(name) always contain full names. function: filters folders extracts names(name) path column. informs size files downloaded/read asks confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"","code":"cloud_s3_prep_bulk(   content,   what = c(\"read\", \"upload\", \"download\"),   safe_size = 5e+07,   quiet = FALSE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"content (data.frame) Output cloud_s3_ls() done content, either \"read\" \"download\". affects messages look. safe_size considered safe size bytes download bulk. show additional caution message case accidentally run bulk reading folder gigabytes data. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"Transformed content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a file from S3 — cloud_s3_read","title":"Read a file from S3 — cloud_s3_read","text":"Retrieves reads file project's S3 folder. default, function attempts determine appropriate reading function based file's extension. However, can specify custom reading function necessary.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a file from S3 — cloud_s3_read","text":"","code":"cloud_s3_read(file, fun = NULL, ..., root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a file from S3 — cloud_s3_read","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun custom reading function. NULL (default), appropriate reading function inferred based file's extension. ... Additional arguments pass reading function fun. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a file from S3 — cloud_s3_read","text":"content file read S3, additional attributes containing metadata file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Read a file from S3 — cloud_s3_read","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a file from S3 — cloud_s3_read","text":"","code":"if (FALSE) { # interactive() # provided there are folders called \"data\" and \"models\" in the root of your # project's main S3 folder and they contain the files mentioned below cloud_s3_read(\"data/mtcars.csv\") cloud_s3_read(\"models/random_forest.rds\") cloud_s3_read(\"data/dm.sas7bdat\", fun = haven::read_sas) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Read Contents from S3 — cloud_s3_read_bulk","title":"Bulk Read Contents from S3 — cloud_s3_read_bulk","text":"function facilitates bulk reading multiple files project's designated S3 folder. using cloud_s3_ls, can obtain dataframe detailing contents S3 folder. Applying cloud_s3_read_bulk dataframe allows read listed files named list. function , default, infer appropriate reading method based file's extension. However, specific reading function provided via fun parameter, applied uniformly files, may suitable diverse file types.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Read Contents from S3 — cloud_s3_read_bulk","text":"","code":"cloud_s3_read_bulk(content, fun = NULL, ..., quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Read Contents from S3 — cloud_s3_read_bulk","text":"content (data.frame) Output cloud_s3_ls() fun custom reading function. NULL (default), appropriate reading function inferred based file's extension. ... Additional arguments pass reading function fun. quiet caution messages may turned setting parameter TRUE. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Read Contents from S3 — cloud_s3_read_bulk","text":"named list element corresponds content file S3. names list elements derived file names.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk Read Contents from S3 — cloud_s3_read_bulk","text":"","code":"if (FALSE) { # interactive() # provided there's a folder called \"data\" in the root of the project's main # S3 folder, and it contains csv files data_lst <-    cloud_s3_ls(\"data\") |>     filter(type == \"csv\")  |>     cloud_s3_read_bulk()    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a local file to S3 — cloud_s3_upload","title":"Upload a local file to S3 — cloud_s3_upload","text":"Uploads local file project's directory corresponding location within project's S3 root folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a local file to S3 — cloud_s3_upload","text":"","code":"cloud_s3_upload(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a local file to S3 — cloud_s3_upload","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a local file to S3 — cloud_s3_upload","text":"Invisibly returns NULL successfully uploading file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a local file to S3 — cloud_s3_upload","text":"","code":"if (FALSE) { # interactive() # create a toy csv file dir.create(\"toy_data\") write.csv(mtcars, \"toy_data/mtcars.csv\")  # uploads toy_data/mtcars.csv to 'data' subfolder of project's S3 folder cloud_s3_upload(\"toy_data/mtcars.csv\")  # clean up unlink(\"toy_data\", recursive = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","title":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","text":"function facilitates bulk uploading multiple files local project folder project's designated S3 folder. using cloud_local_ls, can obtain dataframe detailing contents local folder. Applying cloud_s3_upload_bulk dataframe allows upload listed files S3.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","text":"","code":"cloud_s3_upload_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bulk Upload Files to S3 — cloud_s3_upload_bulk","text":"","code":"if (FALSE) { # interactive() # create toy plots: 2 png's and 1 jpeg dir.create(\"toy_plots\") png(\"toy_plots/plot1.png\"); plot(rnorm(100)); dev.off() png(\"toy_plots/plot2.png\"); plot(hist(rnorm(100))); dev.off() png(\"toy_plots/plot3.jpeg\"); plot(hclust(dist(USArrests), \"ave\")); dev.off()  # upload only the two png's cloud_local_ls(\"toy_plots\")  |>    dplyr::filter(type == \"png\")  |>    cloud_s3_upload_bulk()  # clean up unlink(\"toy_plots\", recursive = TRUE)    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an object to S3 — cloud_s3_write","title":"Write an object to S3 — cloud_s3_write","text":"Saves R object designated location project's S3 storage. custom writing function specified, function infer appropriate writing method based file's extension.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an object to S3 — cloud_s3_write","text":"","code":"cloud_s3_write(x, file, fun = NULL, ..., local = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an object to S3 — cloud_s3_write","text":"x R object written S3. file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun custom writing function. NULL (default), appropriate writing function inferred based file's extension. ... Additional arguments pass writing function fun. local Logical. TRUE, local copy file also created specified path. Default FALSE. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write an object to S3 — cloud_s3_write","text":"Invisibly returns NULL successfully writing object S3.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Write an object to S3 — cloud_s3_write","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an object to S3 — cloud_s3_write","text":"","code":"if (FALSE) { # interactive() # write mtcars dataframe to mtcars.csv in data folder cloud_s3_write(mtcars, \"data/mtcars.csv\") cloud_s3_write(random_forest, \"models/random_forest.rds\")  # provide custom writing function with parameters  cloud_s3_write(c(\"one\", \"two\"), \"text/count.txt\", writeLines, sep = \"\\n\\n\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","title":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","text":"function allows bulk writing multiple R objects project's designated S3 folder. prepare list objects writing, use cloud_object_ls, generates dataframe listing objects intended destinations format akin output cloud_s3_ls. default, function determines appropriate writing method based file's extension. However, specific writing function provided via fun parameter, applied files, may ideal dealing variety file types.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","text":"","code":"cloud_s3_write_bulk(   content,   fun = NULL,   ...,   local = FALSE,   quiet = FALSE,   root = NULL )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","text":"content (data.frame) output cloud_object_ls() fun custom writing function. NULL (default), appropriate writing function inferred based file's extension. ... Additional arguments pass writing function fun. local Logical. TRUE, local copy file also created specified path. Default FALSE. quiet caution messages may turned setting parameter TRUE. root S3 path project root. serves reference point relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","text":"Invisibly returns input content dataframe.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write multiple objects to S3 in bulk — cloud_s3_write_bulk","text":"","code":"if (FALSE) { # interactive() # write two csv files: data/df_mtcars.csv and data/df_iris.csv cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) |>   cloud_s3_write_bulk()    }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate file path for cloud functions — cloud_validate_file_path","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"Makes sure file path passed cloud function right format.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"","code":"cloud_validate_file_path(file, error = TRUE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. error TRUE (default), throws error file valid file path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"Either TRUE FALSE error FALSE. Either TRUE error error TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloudfs-package.html","id":null,"dir":"Reference","previous_headings":"","what":"cloudfs: Streamlined Interface to Interact with Cloud Storage Platforms — cloudfs-package","title":"cloudfs: Streamlined Interface to Interact with Cloud Storage Platforms — cloudfs-package","text":"unified interface simplifying cloud storage interactions, including uploading, downloading, reading, writing files, functions 'Google Drive' (https://www.google.com/drive/) 'Amazon S3' (https://aws.amazon.com/s3/).","code":""},{"path":[]},{"path":"https://g6t.github.io/cloudfs/reference/cloudfs-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"cloudfs: Streamlined Interface to Interact with Cloud Storage Platforms — cloudfs-package","text":"Maintainer: Iaroslav Domin iaroslav@gradientmetrics.com Authors: Stefan Musch stefan@gradientmetrics.com Michal Czyz michal@gradientmetrics.com Emmanuel Ugochukwu emmanuel@gradientmetrics.com contributors: Gradient Metrics [copyright holder, funder]","code":""},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from DESCRIPTION file — proj_desc_get","title":"Extract values from DESCRIPTION file — proj_desc_get","text":"Extract values DESCRIPTION file","code":""},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from DESCRIPTION file — proj_desc_get","text":"","code":"proj_desc_get(key, project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from DESCRIPTION file — proj_desc_get","text":"key Character. field search DESCRIPTION file. project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract values from DESCRIPTION file — proj_desc_get","text":"string value extracted DESCRIPTION field.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate project's DESCRIPTION file — validate_desc","title":"Validate project's DESCRIPTION file — validate_desc","text":"Checks DESCRIPTION file exists project folder. case, proposes create DESCRIPTION file template.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate project's DESCRIPTION file — validate_desc","text":"","code":"validate_desc(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate project's DESCRIPTION file — validate_desc","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate project's DESCRIPTION file — validate_desc","text":"Either TRUE error.","code":""},{"path":[]},{"path":"https://g6t.github.io/cloudfs/news/index.html","id":"cloudfs-012","dir":"Changelog","previous_headings":"","what":"cloudfs 0.1.2","title":"cloudfs 0.1.2","text":"CRAN release: 2023-10-18 Initial version.","code":""}]
