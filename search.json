[{"path":"https://g6t.github.io/cloudfs/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 cloudfs authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"project-assets","dir":"Articles","previous_headings":"Introduction","what":"Project assets","title":"cloudfs package overview","text":"terms digital assets data analysis project usually can divided three parts: input data: primarily tabular data (csv, sav, …), limited ; can e.g. raw text code sources: R Rmd scripts data processing happens outputs: spreadsheets, plots, presentations derived input data ’s common segregate storage code sources, input data, outputs. Code sources typically housed platforms like GitHub, optimized version control collaboration. Storing large input data platforms can inefficient. Systems like git can become overwhelmed tracking changes sizable data files. Hence, cloud storage solutions like Amazon S3 commonly chosen ability handle vast datasets. Outputs, results destined sharing review, usually stored separately code. Platforms like Google Drive preferred purpose. user-friendly interfaces easy sharing options ensure stakeholders can readily access review results.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"cloud-roots","dir":"Articles","previous_headings":"Introduction","what":"Cloud roots","title":"cloudfs package overview","text":"context, won’t differentiate assets inputs outputs. ’ll simply refer artifacts, setting apart code. package includes functions working S3 Google Drive. operations, use term cloud root denote primary folder either platform, holds project’s artifacts. Typically, root folder contains subfolders like “plots”, “data”, “results”, though exact structure isn’t crucial. project can root S3, Google Drive, platforms simultaneously.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"motivation","dir":"Articles","previous_headings":"Introduction","what":"Motivation","title":"cloudfs package overview","text":"Consider typical task: uploading file Amazon S3 using aws.s3 package illustrative example. Imagine ’re attempting upload R model saved RDS file located “models/glm.rds”. file destined “project-1” directory within “project-data” bucket S3, representing dedicated S3 root project: Note following: Location Redundancy: Given project’s primary interactions “project-1” folder “project-data” bucket, ’re consistently faced specifying static location. Path Duplication: local system S3 use matching paths: “models/glm.rds”. uniformity typically practical making exceptions. Given repetitive nature code, ’s room streamlined approach. cloudfs package comes . set , uploading becomes much neater:","code":"aws.s3::put_object(   bucket = \"project-data\",   object = \"project-1/models/glm.rds\",   file = \"models/glm.rds\" ) cloud_s3_upload(\"models/glm.rds\")"},{"path":[]},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"setting-up-a-root","dir":"Articles","previous_headings":"Package walkthrough","what":"Setting up a root","title":"cloudfs package overview","text":"begin working cloudfs package R project, first set cloud root. S3 use cloud_s3_attach(), Google Drive, use cloud_drive_attach() function. Let’s set Google Drive root: Upon execution, ’ll prompted input URL intended Google Drive folder serve project’s root. location registered project’s DESCRIPTION file. conveniently access directory future, execute cloud_drive_browse().","code":"library(cloudfs) cloud_drive_attach()"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"types-of-interactions","dir":"Articles","previous_headings":"Package walkthrough","what":"Types of interactions","title":"cloudfs package overview","text":"Now let’s talk actual interactions cloud storage. Data transfer actions can categorized two parameters: direction – whether ’re uploading data cloud retrieving data . file R object – using cloudfs, can upload download files cloud storages also directly read write objects cloud. cloudfs functions moving files use “upload” “download” names. Functions direct reading writing use “read” “write”. S3-specific functions contain “s3”, Google Drive ones use “drive”.","code":""},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"practical-examples","dir":"Articles","previous_headings":"Package walkthrough","what":"Practical Examples","title":"cloudfs package overview","text":", ’ll demonstrate hands-application cloudfs functions data transfer. Upon successfully completing cloud_drive_attach() process, project associated designated Google Drive root. initial step, create save ggplot scatterplot local PNG file purpose demonstration. upload file Google Drive, execute: invoking cloud_drive_ls() function, can view automatically created “plots” folder console. inspect contents folder, currently contains single PNG file, use cloud_drive_ls(\"plots\"). access folder Google Drive, execute cloud_drive_browse(\"plots\"). directly view scatterplot, use cloud_drive_browse(\"plots/scatterplot.png\"). cloudfs, can directly write content cloud storage, bypassing manual creation local files. file generation process remains transparent user. First, let’s compute summary mtcars dataframe: export summary spreadsheet, simply specify desired file path appropriate extension. method writing inferred extension: view resulting spreadsheet Google Drive, execute cloud_drive_browse(\"results/mtcars_summary.xlsx\"). Just wrote summary xlsx file, can also read using cloud_drive_read(\"results/mtcars_summary.xlsx\"). ’s noteworthy writing reading methods determined automatically based file extension. instance, “.xlsx” utilizes writexl::write_xlsx() reading, whereas “.csv” employs readr::write_csv. comprehensive list default methods available documentation cloud_drive_write() cloud_drive_read() functions. Additionally, cloudfs offers flexibility allowing custom writing reading methods. instance, earlier scatterplot written directly Google Drive, bypassing local file generation:","code":"library(ggplot2) p <- ggplot(mtcars, aes(mpg, disp)) + geom_point() if (!dir.exists(\"plots\")) dir.create(\"plots\") ggsave(plot = p, filename = \"plots/scatterplot.png\") cloud_drive_upload(\"plots/scatterplot.png\") library(dplyr, quietly = TRUE) summary_df <-    mtcars %>%    group_by(cyl) %>%    summarise(across(disp, mean)) cloud_drive_write(summary_df, \"results/mtcars_summary.xlsx\") cloud_drive_write(   p, \"plots/scatterplot.png\",   fun = \\(x, file)      ggsave(plot = x, filename = file) )"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"operations-on-multiple-files-at-once","dir":"Articles","previous_headings":"Package walkthrough","what":"Operations on multiple files at once","title":"cloudfs package overview","text":"Suppose multiple CSV files uploaded “data” folder intend download locally. Instead invoking cloud_s3_download() file, efficient approach available. first, let’s generate sample files demonstration purposes. Listing contents “data” folder gives us following: cloudfs offers bulk functions simplify management multiple files simultaneously. instance, download files listed use cloud_drive_download_bulk(): action automatically downloads datasets local “data” directory, replicating structure Google Drive. read several CSV files “data” folder Google Drive consolidated list, execute: upload collection objects, ggplot visualizations, Google Drive, first group named list. , utilize cloud_object_ls() function generate dataframe akin output cloud_drive_ls(). Finally, execute cloud_drive_write_bulk() complete upload. bulk uploads local files Google Drive, utilize cloud_local_ls() function. instance, upload PNG files local “plots” directory Google Drive:","code":"cloud_drive_write(datasets::airquality, \"data/airquality.csv\") cloud_drive_write(datasets::trees, \"data/trees.csv\") cloud_drive_write(datasets::beaver1, \"data/beaver1.csv\") cloud_drive_ls(\"data\") #> # A tibble: 3 × 5 #>   name           type  last_modified       size_b id       #>   <chr>          <chr> <dttm>               <dbl> <drv_id> #> 1 airquality.csv csv   2023-09-12 08:04:46   2890 1CXTi1A… #> 2 beaver1.csv    csv   2023-09-12 08:04:50   1901 1Fg4s1O… #> 3 trees.csv      csv   2023-09-12 08:04:48    400 1vDYBVt… cloud_drive_ls(\"data\") %>%    cloud_drive_download_bulk() all_data <-    cloud_drive_ls(\"data\") %>%    cloud_drive_read_bulk() library(ggplot2) p1 <- ggplot(mtcars, aes(mpg, disp)) + geom_point() p2 <- ggplot(mtcars, aes(cyl)) + geom_bar()  plots_list <-    list(\"plot_1\" = p1, \"plot_2\" = p2) %>%    cloud_object_ls(path = \"plots\", extension = \"png\", suffix = \"_newsletter\")  plots_list %>%    cloud_drive_write_bulk(fun = \\(x, file) ggsave(plot = x, filename = file)) cloud_local_ls(\"plots\") %>%    filter(type == \"png\") %>%    cloud_drive_upload_bulk()"},{"path":"https://g6t.github.io/cloudfs/articles/cloudfs.html","id":"s3-functions","dir":"Articles","previous_headings":"Package walkthrough","what":"S3 functions","title":"cloudfs package overview","text":"Amazon S3 interactions, offer parallel set functions similar designed Google Drive. dedicated S3 functions easily identifiable, beginning prefix cloud_s3_.","code":""},{"path":"https://g6t.github.io/cloudfs/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Iaroslav Domin. Author, maintainer. Stefan Musch. Author. Michal Czyz. Author. Emmanuel Ugochukwu. Author. . Copyright holder, funder.","code":""},{"path":"https://g6t.github.io/cloudfs/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Domin , Musch S, Czyz M, Ugochukwu E (2023). cloudfs: Streamlined interface interact cloud storage platforms. https://g6t.github.io/cloudfs/, https://github.com/g6t/cloudfs.","code":"@Manual{,   title = {cloudfs: Streamlined interface to interact with cloud storage platforms},   author = {Iaroslav Domin and Stefan Musch and Michal Czyz and Emmanuel Ugochukwu},   year = {2023},   note = {https://g6t.github.io/cloudfs/, https://github.com/g6t/cloudfs}, }"},{"path":"https://g6t.github.io/cloudfs/index.html","id":"cloudfs-","dir":"","previous_headings":"","what":"Streamlined interface to interact with cloud storage platforms","title":"Streamlined interface to interact with cloud storage platforms","text":"cloudfs R package offers unified interface simplifying cloud storage interactions, including uploading, downloading, reading, writing files, functions Google Drive Amazon S3.","code":""},{"path":"https://g6t.github.io/cloudfs/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Streamlined interface to interact with cloud storage platforms","text":"","code":"remotes::install_github(\"g6t/cloudfs\")"},{"path":"https://g6t.github.io/cloudfs/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Streamlined interface to interact with cloud storage platforms","text":"Relative path simplicity Use paths relative project’s main cloud folder. Unified Interface Google Drive S3 Uploading S3? process just straightforward. Extension-aware functions package automatically selects right read write function based file extension, simplifying interactions. Effortless cloud navigation Open folders browser: list contents console: Bulk File Management Easily read data files S3 folder one go.","code":"cloud_drive_upload(\"plots/my_plot.png\") cloud_s3_upload(\"plots/my_plot.png\") cloud_s3_write(glmnet_model, \"models/glmnet.rds\") cloud_drive_browse(\"my_folder\") cloud_s3_ls(\"data\") cloud_s3_ls(\"data\") %>% cloud_s3_read_bulk()"},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":null,"dir":"Reference","previous_headings":"","what":"Capture Arguments — check_args","title":"Capture Arguments — check_args","text":"Helper catch arguments.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Capture Arguments — check_args","text":"","code":"check_args(...)"},{"path":"https://g6t.github.io/cloudfs/reference/check_args.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Capture Arguments — check_args","text":"... unqouted arguments names","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Argument is Single TRUE or FALSE — check_bool","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"Check argument single TRUE FALSE. option possible allow NULL value alt_null = TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"","code":"check_bool(x, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"x Function argument asserted. alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_name} message body (e.g. \"{x_name}\"); pass class arguments failed test using {wrong_class} message body (e.g. \"{wrong_class} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"argument single TRUE FALSE (optionally NULL) returns invisible NULL. Otherwise function throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_bool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Argument is Single TRUE or FALSE — check_bool","text":"","code":"c1 <- c(\"x\", \"y\") n1 <- c(1,3,4) n2 <- c(1.5, 2.5) i1 <- 1L nl1 <- NULL l1 <- FALSE l2 <- c(FALSE, TRUE) if (FALSE) { check_bool(c1) check_bool(nl1) check_bool(nl1, alt_null = TRUE) check_bool(n2, alt_null = TRUE) check_bool(i1) check_bool(l1) check_bool(l2) }"},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Argument's Class — check_class","title":"Check Argument's Class — check_class","text":"Check argument proper class.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Argument's Class — check_class","text":"","code":"check_class(x, arg_class, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Argument's Class — check_class","text":"x Function argument asserted. arg_class Class name. Usually \"character\", \"numeric\", \"data.frame\", etc. alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_names} message body (e.g. \"{x_names}\"); pass tested class using {arg_class} message body (e.g. \"want {arg_class})\" pass classes arguments failed test using {wrong_class} message body (e.g. \"{wrong_class} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Argument's Class — check_class","text":"argument class arg_class returns invisible NULL. Otherwise function throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Argument's Class — check_class","text":"","code":"c1 <- c(\"x\", \"y\") n1 <- c(1,3,4) n2 <- c(1.5, 2.5) i1 <- 1L df1 <- data.frame(x = 1:5, y = 6:10) new_class <- structure(\"new class\", class= c(\"character\", \"new class\")) nl1 <- NULL if (FALSE) { check_class(c1, arg_class = \"character\") check_class(c1, arg_class = \"numeric\") check_class(df1, arg_class = \"data.frame\") check_class(   new_class, arg_class = \"tbl_df\",   add_msg = \"{.arg {x_name}} with {.cls {wrong_class}} not {.cls {arg_class}}\" ) check_class(nl1, arg_class = \"character\") check_class(nl1, arg_class = \"character\", alt_null = TRUE) check_class(n2, arg_class = \"character\", alt_null = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Argument is of Proper Length — check_length","title":"Check if Argument is of Proper Length — check_length","text":"TODO.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Argument is of Proper Length — check_length","text":"","code":"check_length(x, arg_length = 1L, alt_null = FALSE, add_msg = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Argument is of Proper Length — check_length","text":"x Function arguments asserted. arg_length Integer. Length argument, scalars take value 1 (default). alt_null Logical. argument accept NULL value. add_msg additional message can printed standard function error message. can: pass names arguments failed test using {x_name} message body (e.g. \"{wrong_names}\"); pass tested length using {arg_length} message body (e.g. \"want {arg_length})\" pass lengths arguments failed test using {wrong_length} message body (e.g. \"{wrong_lengths} wrong\")","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Argument is of Proper Length — check_length","text":"Returns invisible NULL argument asserted length, otherwise throw error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_length.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Argument is of Proper Length — check_length","text":"","code":"x1 <- 2 x2 <- c(\"x\", \"y\") nl1 <- NULL if (FALSE) { check_length(x1, arg_length = 1L) check_length(x2, arg_length = 1L) check_length(x1, arg_length = 2L) check_length(   x1, arg_length = 2L, add_msg = \"{.arg {x_name}} should be short\" ) check_length(   x1, arg_length = 2L, alt_null = TRUE, add_msg = \"{.arg {x_name}} should be short\" ) check_length(   nl1, arg_length = 2L, alt_null = TRUE, add_msg = \"{.arg {x_name}} should be short\" ) }"},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":null,"dir":"Reference","previous_headings":"","what":"Return check_null Value — check_null_cond","title":"Return check_null Value — check_null_cond","text":"Check alt_null argument TRUE FALSE. FALSE return FALSE. argument TRUE check x argument NULL return logical value.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return check_null Value — check_null_cond","text":"","code":"check_null_cond(x, alt_null)"},{"path":"https://g6t.github.io/cloudfs/reference/check_null_cond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return check_null Value — check_null_cond","text":"x Argument check NULL. alt_null Logical. TRUE check x NULL.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Function Argument is Scalar — check_scalar","title":"Check if Function Argument is Scalar — check_scalar","text":"function check argument proper class length 1.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Function Argument is Scalar — check_scalar","text":"","code":"check_scalar(..., arg_class, alt_null = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Function Argument is Scalar — check_scalar","text":"... Function argument asserted. arg_class Class name. Usually \"character\", \"numeric\", \"data.frame\", etc. alt_null Logical. argument accept NULL value.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Function Argument is Scalar — check_scalar","text":"Invisible NULL assertion TRUE, otherwise error message.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/check_scalar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Function Argument is Scalar — check_scalar","text":"","code":"# Variables values to test char_s <- \"test\" char_v <- c(\"test\", \"variable\") num_s <- 1.5 num_v <- c(2, 1.5, 3.33) logical_s <- TRUE logical_v <- c(FALSE, FALSE) int_s <- 1L int_v <- c(3L, 33L) if (FALSE) { # Assert Scalar Character check_scalar(char_s, arg_class = \"character\") check_scalar(char_v, arg_class = \"character\") check_scalar(num_s, arg_class = \"character\") check_scalar(logical_s, arg_class = \"character\")  # Assert Scalar Numeric check_scalar(num_s, arg_class = \"numeric\") check_scalar(num_v, arg_class = \"numeric\") check_scalar(int_s, arg_class = \"numeric\", alt_null = TRUE) check_scalar(logical_v, arg_class = \"numeric\")  # Assert Scalar Logical check_scalar(logical_s, arg_class = \"logical\") check_scalar(logical_v, arg_class = \"logical\") check_scalar(char_s, arg_class = \"logical\", alt_null = TRUE) check_scalar(int_v, arg_class = \"logical\")  #' # Assert Scalar Integer check_scalar(int_s, arg_class = \"integer\") check_scalar(int_v,  arg_class = \"integer\", alt_null = TRUE) check_scalar(num_s, arg_class = \"integer\") check_scalar(logical_v, arg_class = \"integer\", alt_null = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":null,"dir":"Reference","previous_headings":"","what":"User Interface: Ask a Yes/No question — cli_yeah","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"function inspired (mostly copied ) usethis::ui_yeah function. purpose ask user yes/question. differences : limited answer options customization. done purpose standardize command line dialogues code. uses cli package hood, cli rich text formatting possible.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"","code":"cli_yeah(x, straight = FALSE, .envir = parent.frame())"},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"x Question display. straight (logical) Ask straight Yes/question? default (FALSE), two different \"\" options one \"yes\" option sampled pool variants. words behaves just like usethis::ui_yeah default parameter setup. straight = TRUE, shows \"Yes\" \"\", literally. .envir Environment evaluate glue expressions .","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"(logical) Returns TRUE user selects \"yes\" option FALSE otherwise, .e. user selects \"\" option refuses make selection (cancels).","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cli_yeah.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"User Interface: Ask a Yes/No question — cli_yeah","text":"","code":"if (FALSE) { cli_yeah(\"Is this {.strong true}?: {.code 2+2 == 4}\") cli_yeah(\"{.field Yes} or {.field No}?\", straight = TRUE) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":null,"dir":"Reference","previous_headings":"","what":"Attach Google Drive folder to project — cloud_drive_attach","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"cloud_drive_attach() function designed add field DESCRIPTION file project, uniquely identifies location project's folder Google Drive. function prompts user visit Google Drive website (https://drive.google.com/) can find create dedicated folder project. user located created desired folder, can copy URL folder web browser paste R console. function parses URL populates corresponding field (cloudfs.drive) DESCRIPTION file string represents location project Google Drive.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"","code":"cloud_drive_attach(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_attach.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Attach Google Drive folder to project — cloud_drive_attach","text":"","code":"if (FALSE) cloud_drive_attach()"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse project's Google Drive folder — cloud_drive_browse","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"Opens project's Google Drive folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"","code":"cloud_drive_browse(path = \"\", root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"path (optional) Path inside Google Drive folder open. default, path = \"\", navigates root level project's folder. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_browse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse project's Google Drive folder — cloud_drive_browse","text":"","code":"if (FALSE) { cloud_drive_browse() cloud_drive_browse(\"models/kmeans\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a file from Google Drive to local project folder — cloud_drive_download","title":"Download a file from Google Drive to local project folder — cloud_drive_download","text":"Downloads file project's Google Drive folder local project folder saves preserving folder structure.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a file from Google Drive to local project folder — cloud_drive_download","text":"","code":"cloud_drive_download(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a file from Google Drive to local project folder — cloud_drive_download","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download a file from Google Drive to local project folder — cloud_drive_download","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a file from Google Drive to local project folder — cloud_drive_download","text":"","code":"if (FALSE) { # downloads data/demo.csv from project's Google Drive folder # and saves it to local 'data' folder cloud_drive_download(\"data/demo.csv\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Google Drive contents in bulk — cloud_drive_download_bulk","title":"Download Google Drive contents in bulk — cloud_drive_download_bulk","text":"cloud_drive_ls function returns dataframe contents Google Drive folder. cloud_drive_download_bulk can applied dataframe download listed files. workflow mind call cloud_drive_ls(), use dplyr verbs keep files need call cloud_drive_download_bulk result.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Google Drive contents in bulk — cloud_drive_download_bulk","text":"","code":"cloud_drive_download_bulk(content, quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Google Drive contents in bulk — cloud_drive_download_bulk","text":"content (data.frame) Output cloud_drive_ls() quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_download_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Google Drive contents in bulk — cloud_drive_download_bulk","text":"","code":"if (FALSE) { cloud_drive_ls(\"data\") %>%    filter(type == \"csv\") %>%    cloud_drive_download_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Google Drive folder based on a path — cloud_drive_find_path","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"Given Google Drive id pointing folder relative path inside folder, returns id object (file folder) corresponding path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"","code":"cloud_drive_find_path(root, path = \"\", create = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"root ID folder start search . path Relative location respect root folder. create Create folders describing path exist? Default FALSE default function throws error path found. TRUE, function create missing subdirectories. Note object deepest level always created folder. E.g. path = \"models/kmeans/model.Rds\" \"model.Rds\" missing, function create folder name.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_find_path.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Google Drive folder based on a path — cloud_drive_find_path","text":"","code":"if (FALSE) { cloud_drive_find_path(\"1ul0MYeHb0nJtnuaPinKV1WtH0n3igmN2\", \"models/kmeans\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of Project's Google Drive Folder — cloud_drive_ls","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"Prints names, timestamps sizes files folders inside Google Drive folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"","code":"cloud_drive_ls(path = \"\", recursive = FALSE, full_names = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"path (optional) Path inside Google Drive folder list contents subfolders. default, path = \"\", lists root-level files folders. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of Project's Google Drive Folder — cloud_drive_ls","text":"","code":"if (FALSE) { # list only root-level files and folders cloud_drive_ls()   # list all files in all nested folders cloud_drive_ls(recursive = TRUE)  # list contents of \"plots/barplots\" subfolder cloud_drive_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"cloud_drive_ls returns dataframe contents Google Drive folder. dataframe name, type id columns. name may either full names short names (depending full_names parameter cloud_drive_ls), names(name) always contain full names. function: filters folders extracts names(name) path column. informs size files downloaded/read asks confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"","code":"cloud_drive_prep_bulk(   content,   what = c(\"read\", \"download\"),   safe_size = 5e+07,   quiet = FALSE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Google Drive content dataframe to be used by bulk\ndownload/read functions — cloud_drive_prep_bulk","text":"content (data.frame) Output cloud_drive_ls() done content, either \"read\" \"download\". affects messages look. safe_size considered safe size bytes download bulk. show additional caution message case accidentally run bulk reading folder gigabytes data. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read an object from Google Drive — cloud_drive_read","title":"Read an object from Google Drive — cloud_drive_read","text":"Reads file project's Google Drive. function tries guess appropriate reading function based file name, can also provide function needed.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read an object from Google Drive — cloud_drive_read","text":"","code":"cloud_drive_read(file, fun = NULL, ..., root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read an object from Google Drive — cloud_drive_read","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun Reading function. default NULL means attempted guess appropriate reading function file extension. ... parameters pass fun. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Read an object from Google Drive — cloud_drive_read","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read an object from Google Drive — cloud_drive_read","text":"","code":"if (FALSE) { cloud_drive_read(\"data/mtcars.csv\") cloud_drive_read(\"models/random_forest.rds\") cloud_drive_read(\"data/dm.sas7bdat\", fun = haven::read_sas) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Read Google Drive contents in bulk — cloud_drive_read_bulk","title":"Read Google Drive contents in bulk — cloud_drive_read_bulk","text":"cloud_drive_ls function returns dataframe contents Google Drive folder. cloud_drive_read_bulk can applied dataframe read listed files named list. attempted guess reading function file extensions. can pass reading function manually setting fun parameter, means files read using one function. fact, probably reading multiple files different types bulk. workflow mind call cloud_drive_ls(), use dplyr verbs keep files need call cloud_drive_read_bulk result. Note need filter folders -- done automatically.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read Google Drive contents in bulk — cloud_drive_read_bulk","text":"","code":"cloud_drive_read_bulk(content, fun = NULL, ..., quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read Google Drive contents in bulk — cloud_drive_read_bulk","text":"content (data.frame) Output cloud_drive_ls() fun Reading function. default NULL means attempted guess appropriate reading function file extension. ... parameters pass fun. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_read_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read Google Drive contents in bulk — cloud_drive_read_bulk","text":"","code":"if (FALSE) { data_lst <-    cloud_drive_ls(\"data\") %>%    filter(type == \"csv\") %>%    cloud_drive_read_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"Finds spreadsheet path relative project root. Applies googlesheets4::range_autofit() sheet.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"","code":"cloud_drive_spreadsheet_autofit(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_spreadsheet_autofit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatically resize all columns in a google spreadsheet — cloud_drive_spreadsheet_autofit","text":"","code":"if (FALSE) { cloud_drive_write(mtcars, \"results/mtcars.xlsx\") cloud_drive_spreadsheet_autofit(\"results/mtcars.xlsx\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a local file to Google Drive — cloud_drive_upload","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"Uploads file project's folder corresponding location project's Google Drive folder","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"","code":"cloud_drive_upload(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"Google Drive file structure different usual file structure like e.g. Linux Windows. folder Google Drive can two child folders name. Google Drive marks files folders -called id values distinguish . values always unique. can see browser URL example. concept \"name\" first place convenience end user. setup relative file path may correspond multiple files folders. function however works assumption relative path pass defines strictly one object. ambiguity throws error.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a local file to Google Drive — cloud_drive_upload","text":"","code":"if (FALSE) { # uploads data/demo.csv to 'data' subfolder of project's Google Drive folder cloud_drive_upload(\"data/demo.csv\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload files to Google Drive in bulk — cloud_drive_upload_bulk","title":"Upload files to Google Drive in bulk — cloud_drive_upload_bulk","text":"cloud_local_ls function returns dataframe contents local project folder. cloud_drive_upload_bulk can applied dataframe upload listed files. workflow mind call cloud_local_ls(), use dplyr verbs keep files need call cloud_drive_upload_bulk result.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload files to Google Drive in bulk — cloud_drive_upload_bulk","text":"","code":"cloud_drive_upload_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload files to Google Drive in bulk — cloud_drive_upload_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_upload_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload files to Google Drive in bulk — cloud_drive_upload_bulk","text":"","code":"if (FALSE) { cloud_local_ls(\"plots\") %>%    filter(type == \"png\") %>%    cloud_drive_upload_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an object to Google Drive — cloud_drive_write","title":"Write an object to Google Drive — cloud_drive_write","text":"Writes R object file project's Google Drive folder. cases expected used writing data.frames csv, sav tabular data formats Google Drive. also possible save R object proper saving function provided. tries guess suitable writing function output file name possible.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an object to Google Drive — cloud_drive_write","text":"","code":"cloud_drive_write(x, file, fun = NULL, ..., local = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an object to Google Drive — cloud_drive_write","text":"x R object (e.g. data frame) write Google Drive. file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun function write file cloud location x file path passed (order). default, fun = NULL, attempted find appropriate writing function based file extension. ... parameters pass fun local (logical) TRUE, additionally create local file corresponding path. Default FALSE. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Write an object to Google Drive — cloud_drive_write","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an object to Google Drive — cloud_drive_write","text":"","code":"if (FALSE) { # write mtcars dataframe to mtcars.csv in data folder cloud_drive_write(mtcars, \"data/mtcars.csv\") cloud_drive_write(random_forest, \"models/random_forest.rds\")  # provide custom writing function with parameters  cloud_drive_write(c(\"one\", \"two\"), \"text/count.txt\", writeLines, sep = \"\\n\\n\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Write objects to Google Drive in bulk — cloud_drive_write_bulk","title":"Write objects to Google Drive in bulk — cloud_drive_write_bulk","text":"Given named list objects cloud_object_ls function returns dataframe similar output cloud_local_ls cloud_drive_ls. cloud_drive_write_bulk can applied dataframe write listed objects S3. attempted guess writing function file extensions. can pass writing function manually setting fun parameter, means files written using one function. fact, probably writing multiple files different types bulk.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write objects to Google Drive in bulk — cloud_drive_write_bulk","text":"","code":"cloud_drive_write_bulk(   content,   fun = NULL,   ...,   local = FALSE,   quiet = FALSE,   root = NULL )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write objects to Google Drive in bulk — cloud_drive_write_bulk","text":"content (data.frame) output cloud_object_ls() fun function write file cloud location x file path passed (order). default, fun = NULL, attempted find appropriate writing function based file extension. ... parameters pass fun local (logical) TRUE, additionally create local file corresponding path. Default FALSE. quiet caution messages may turned setting parameter TRUE. root GoogleDrive id URL project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.drive field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_drive_write_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write objects to Google Drive in bulk — cloud_drive_write_bulk","text":"","code":"if (FALSE) { # write two csv files: data/df_mtcars.csv and data/df_iris.csv cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) %>%  cloud_drive_write_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cloud roots of a project — cloud_get_roots","title":"Get cloud roots of a project — cloud_get_roots","text":"Returns list cloudfs.* roots defined project's DESCRIPTION.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cloud roots of a project — cloud_get_roots","text":"","code":"cloud_get_roots(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_get_roots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cloud roots of a project — cloud_get_roots","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Guess reading function based on file extensions — cloud_guess_read_fun","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"Take look switch call. basically . Returns appropriate function throws error able find one.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"","code":"cloud_guess_read_fun(file)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_read_fun.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Guess reading function based on file extensions — cloud_guess_read_fun","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Guess writing function based on file extensions — cloud_guess_write_fun","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"Take look switch call. basically . Returns appropriate function throws error able find one.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"","code":"cloud_guess_write_fun(file)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_guess_write_fun.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Guess writing function based on file extensions — cloud_guess_write_fun","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of local project folder — cloud_local_ls","title":"List Contents of local project folder — cloud_local_ls","text":"Prints names, timestamps sizes files folders inside local project folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of local project folder — cloud_local_ls","text":"","code":"cloud_local_ls(   path = \"\",   root = \".\",   recursive = FALSE,   full_names = FALSE,   ignore = TRUE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of local project folder — cloud_local_ls","text":"path (optional) Path inside local project folder list contents subfolder. default, path = \"\", lists root-level files folders. root Local path relative consider paths. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. ignore (logical) Currently just ignores \"renv\" folder TRUE. main reason parameter \"renv\" folder usually contains thousands files takes lot time calculate size. potentially may use something like global project-level cloud ignore files akin .gitignore.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_local_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of local project folder — cloud_local_ls","text":"","code":"if (FALSE) { # list only root-level files and folders cloud_local_ls()   # list all files in all nested folders cloud_local_ls(recursive = TRUE)  # list contents of \"plots/barplots\" subfolder cloud_local_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare an ls dataframe for a list of objects — cloud_object_ls","title":"Prepare an ls dataframe for a list of objects — cloud_object_ls","text":"cloud_*_ls functions cloud locations (e.g. cloud_s3_ls) return content dataframes can passed cloud_*_read_bulk cloud_*_download_bulk functions read/download multiple files . similar manner, function takes list objects input produces dataframe can passed cloud_*_write_bulk functions write multiple files .","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare an ls dataframe for a list of objects — cloud_object_ls","text":"","code":"cloud_object_ls(x, path, extension, prefix = \"\", suffix = \"\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare an ls dataframe for a list of objects — cloud_object_ls","text":"x named list. Names may contain letters, digits, spaces, '.', '-', '_' symbols contain trailing leading spaces. path directory write objects . extension File extension (string). prefix, suffix (optional) strings attach beginning end file names.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare an ls dataframe for a list of objects — cloud_object_ls","text":"tibble two columns. object - objects provided name - contains paths objects meant written.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare an ls dataframe for a list of objects — cloud_object_ls","text":"","code":"cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) #> # A tibble: 2 × 3 #>   object         name               type  #>   <named list>   <chr>              <chr> #> 1 <df [32 × 11]> data/df_mtcars.csv csv   #> 2 <df [150 × 5]> data/df_iris.csv   csv"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"cloud_object_ls returns ls-like dataframe named list objects. function used mainly inform user files going written ask confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"","code":"cloud_object_prep_bulk(content, quiet = FALSE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_object_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare object content dataframe to be used by bulk download/read\nfunctions — cloud_object_prep_bulk","text":"content (data.frame) output cloud_object_ls() quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare ls output — cloud_prep_ls","title":"Prepare ls output — cloud_prep_ls","text":"hood ls functions (s3, drive, local) obtain information folder content recursively regardless recursive parameter. needed able calculate last modified time size folders case recursive set FALSE. content presented form dataframe similar see run ls function recursive = TRUE full_names = FALSE. function takes dataframe point : Summarizes non-recursive output recursive FALSE. Appends path names full_names TRUE. Writes full names names name column regardless full_names parameter. Evaluates type column.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare ls output — cloud_prep_ls","text":"","code":"cloud_prep_ls(data, path, recursive, full_names)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_prep_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare ls output — cloud_prep_ls","text":"data ls dataframe assembled internally cloud_ls_* function path path used cloud_ls_* function recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":null,"dir":"Reference","previous_headings":"","what":"Read excel file as a list of dataframes — cloud_read_excel","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"Uses readxl::read_excel hood, reads sheets returns named list dataframes.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"","code":"cloud_read_excel(path)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"path Path xlsx/xslx file","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_read_excel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read excel file as a list of dataframes — cloud_read_excel","text":"","code":"if (FALSE) { data_lst <- cloud_read_excel(\"my_project/data.xlsx\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":null,"dir":"Reference","previous_headings":"","what":"Attach S3 folder to project — cloud_s3_attach","title":"Attach S3 folder to project — cloud_s3_attach","text":"cloud_s3_attach() function used add field DESCRIPTION file project, uniquely identifies location project's folder S3 cloud storage. prompts user visit S3 cloud storage website (https://s3.console.aws.amazon.com/) can find create dedicated folder project. user selected created desired folder, can copy URL folder web browser paste R console. function parses URL populates corresponding field (cloudfs.s3) DESCRIPTION file string represents location project S3 cloud storage.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Attach S3 folder to project — cloud_s3_attach","text":"","code":"cloud_s3_attach(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Attach S3 folder to project — cloud_s3_attach","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_attach.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Attach S3 folder to project — cloud_s3_attach","text":"","code":"if (FALSE) cloud_s3_attach()"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse project's S3 folder — cloud_s3_browse","title":"Browse project's S3 folder — cloud_s3_browse","text":"Opens project's S3 folder browser.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse project's S3 folder — cloud_s3_browse","text":"","code":"cloud_s3_browse(path = \"\", root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse project's S3 folder — cloud_s3_browse","text":"path (optional) Path inside S3 folder open. default, path = \"\", navigates root level project's S3 folder. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_browse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse project's S3 folder — cloud_s3_browse","text":"","code":"if (FALSE) { cloud_s3_browse() cloud_s3_browse(\"data\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a file from S3 to local project folder — cloud_s3_download","title":"Download a file from S3 to local project folder — cloud_s3_download","text":"Downloads file project's S3 folder local project folder saves preserving folder structure.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a file from S3 to local project folder — cloud_s3_download","text":"","code":"cloud_s3_download(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a file from S3 to local project folder — cloud_s3_download","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a file from S3 to local project folder — cloud_s3_download","text":"","code":"if (FALSE) { # downloads data/demo.csv from project's S3 folder # and saves it to local 'data' folder cloud_s3_download(\"data/demo.csv\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Download S3 contents in bulk — cloud_s3_download_bulk","title":"Download S3 contents in bulk — cloud_s3_download_bulk","text":"cloud_s3_ls function returns dataframe contents S3 folder. cloud_s3_download_bulk can applied dataframe download listed files. cloud_s3_download used hood. workflow mind call cloud_s3_ls(), use dplyr verbs keep files need call cloud_s3_download_bulk result.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download S3 contents in bulk — cloud_s3_download_bulk","text":"","code":"cloud_s3_download_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download S3 contents in bulk — cloud_s3_download_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_download_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download S3 contents in bulk — cloud_s3_download_bulk","text":"","code":"if (FALSE) { cloud_s3_ls(\"data\") %>%    filter(type == \"csv\") %>%    cloud_s3_download_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":null,"dir":"Reference","previous_headings":"","what":"List Contents of Project's S3 Folder — cloud_s3_ls","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"Prints names, timestamps sizes files folders inside S3 folder.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"","code":"cloud_s3_ls(path = \"\", recursive = FALSE, full_names = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"path (optional) Path inside S3 folder list contents subfolder. default, path = \"\", lists root-level files folders. recursive (logical) TRUE, lists contents recursively nested subfolders. Default FALSE. full_names (logical) TRUE, folder path appended object names give relative file path. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_ls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Contents of Project's S3 Folder — cloud_s3_ls","text":"","code":"if (FALSE) { # list only root-level files and folders cloud_s3_ls()   # list all files in all nested folders cloud_s3_ls(recursive = TRUE)  # list contents of \"plots/barplots\" subfolder cloud_s3_ls(\"plots/barplots\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"cloud_s3_ls returns dataframe contents S3 folder. dataframe name type columns. name may either full names short names (depending full_names parameter cloud_s3_ls), names(name) always contain full names. function: filters folders extracts names(name) path column. informs size files downloaded/read asks confirmation","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"","code":"cloud_s3_prep_bulk(   content,   what = c(\"read\", \"upload\", \"download\"),   safe_size = 5e+07,   quiet = FALSE )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_prep_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare S3 content dataframe to be used by bulk download/read\nfunctions — cloud_s3_prep_bulk","text":"content (data.frame) Output cloud_s3_ls() done content, either \"read\" \"download\". affects messages look. safe_size considered safe size bytes download bulk. show additional caution message case accidentally run bulk reading folder gigabytes data. quiet caution messages may turned setting parameter TRUE.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read an object from S3 — cloud_s3_read","title":"Read an object from S3 — cloud_s3_read","text":"Reads file project's S3. function tries guess appropriate reading function based file name, can also provide function needed.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read an object from S3 — cloud_s3_read","text":"","code":"cloud_s3_read(file, fun = NULL, ..., root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read an object from S3 — cloud_s3_read","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun Reading function. default NULL means attempted guess appropriate reading function file extension. ... parameters pass fun. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"default-reading-functions","dir":"Reference","previous_headings":"","what":"Default reading functions","title":"Read an object from S3 — cloud_s3_read","text":"identify reading function based file extension .csv: readr::read_csv .json: jsonlite::read_json .rds: base::readRDS .sav: haven::read_sav .xls: cloud_read_excel .xlsx: cloud_read_excel .xml: xml2::read_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read an object from S3 — cloud_s3_read","text":"","code":"if (FALSE) { cloud_s3_read(\"data/mtcars.csv\") cloud_s3_read(\"models/random_forest.rds\") cloud_s3_read(\"data/dm.sas7bdat\", fun = haven::read_sas) }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Read S3 contents in bulk — cloud_s3_read_bulk","title":"Read S3 contents in bulk — cloud_s3_read_bulk","text":"cloud_s3_ls function returns dataframe contents S3 folder. cloud_s3_read_bulk can applied dataframe read listed files named list. cloud_s3_read used hood. attempted guess reading function file extensions. can pass reading function manually setting fun parameter, means files read using one function. fact, probably reading multiple files different types bulk. workflow mind call cloud_s3_ls(), use dplyr verbs keep files need call cloud_s3_read_bulk result. Note need filter folders -- done automatically.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read S3 contents in bulk — cloud_s3_read_bulk","text":"","code":"cloud_s3_read_bulk(content, fun = NULL, ..., quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read S3 contents in bulk — cloud_s3_read_bulk","text":"content (data.frame) Output cloud_s3_ls() fun Reading function. default NULL means attempted guess appropriate reading function file extension. ... parameters pass fun. quiet caution messages may turned setting parameter TRUE. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_read_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read S3 contents in bulk — cloud_s3_read_bulk","text":"","code":"if (FALSE) { data_lst <-    cloud_s3_ls(\"data\") %>%    filter(type == \"csv\") %>%    cloud_s3_read_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a local file to S3 — cloud_s3_upload","title":"Upload a local file to S3 — cloud_s3_upload","text":"Uploads file project's folder corresponding location project's S3 folder","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a local file to S3 — cloud_s3_upload","text":"","code":"cloud_s3_upload(file, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a local file to S3 — cloud_s3_upload","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a local file to S3 — cloud_s3_upload","text":"","code":"if (FALSE) { # uploads data/demo.csv to 'data' subfolder of project's S3 folder cloud_s3_upload(\"data/demo.csv\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload files to S3 in bulk — cloud_s3_upload_bulk","title":"Upload files to S3 in bulk — cloud_s3_upload_bulk","text":"cloud_local_ls function returns dataframe contents local project folder. cloud_s3_upload_bulk can applied dataframe upload listed files. cloud_s3_upload used hood. workflow mind call cloud_local_ls(), use dplyr verbs keep files need call cloud_s3_upload_bulk result.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload files to S3 in bulk — cloud_s3_upload_bulk","text":"","code":"cloud_s3_upload_bulk(content, quiet = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload files to S3 in bulk — cloud_s3_upload_bulk","text":"content (data.frame) Output cloud_s3_ls() quiet caution messages may turned setting parameter TRUE. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_upload_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload files to S3 in bulk — cloud_s3_upload_bulk","text":"","code":"if (FALSE) { cloud_local_ls(\"plots\") %>%    filter(type == \"png\") %>%    cloud_s3_upload_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write an object to S3 — cloud_s3_write","title":"Write an object to S3 — cloud_s3_write","text":"Writes R object file project's S3 folder. cases expected used writing data.frames csv, sav tabular data formats S3. also possible save R object proper saving function provided. tries guess suitable writing function output file name possible.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write an object to S3 — cloud_s3_write","text":"","code":"cloud_s3_write(x, file, fun = NULL, ..., local = FALSE, root = NULL)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write an object to S3 — cloud_s3_write","text":"x R object (e.g. data frame) write S3. file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. fun function write file cloud location x file path passed (order). default, fun = NULL, attempted find appropriate writing function based file extension. ... parameters pass fun local (logical) TRUE, additionally create local file corresponding path. Default FALSE. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"default-writing-functions","dir":"Reference","previous_headings":"","what":"Default writing functions","title":"Write an object to S3 — cloud_s3_write","text":"identify writing function based file extension .csv: readr::write_csv .json: jsonlite::write_json .rds: base::saveRDS .xls: writexl::write_xlsx .xlsx: writexl::write_xlsx .sav: haven::write_sav .xml: xml2::write_xml","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write an object to S3 — cloud_s3_write","text":"","code":"if (FALSE) { # write mtcars dataframe to mtcars.csv in data folder cloud_s3_write(mtcars, \"data/mtcars.csv\") cloud_s3_write(random_forest, \"models/random_forest.rds\")  # provide custom writing function with parameters  cloud_s3_write(c(\"one\", \"two\"), \"text/count.txt\", writeLines, sep = \"\\n\\n\") }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":null,"dir":"Reference","previous_headings":"","what":"Write objects to S3 in bulk — cloud_s3_write_bulk","title":"Write objects to S3 in bulk — cloud_s3_write_bulk","text":"Given named list objects cloud_object_ls function returns dataframe similar output cloud_local_ls cloud_s3_ls. cloud_s3_write_bulk can applied dataframe write listed objects S3. cloud_s3_write used hood. attempted guess writing function file extensions. can pass writing function manually setting fun parameter, means files written using one function. fact, probably writing multiple files different types bulk.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write objects to S3 in bulk — cloud_s3_write_bulk","text":"","code":"cloud_s3_write_bulk(   content,   fun = NULL,   ...,   local = FALSE,   quiet = FALSE,   root = NULL )"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write objects to S3 in bulk — cloud_s3_write_bulk","text":"content (data.frame) output cloud_object_ls() fun function write file cloud location x file path passed (order). default, fun = NULL, attempted find appropriate writing function based file extension. ... parameters pass fun local (logical) TRUE, additionally create local file corresponding path. Default FALSE. quiet caution messages may turned setting parameter TRUE. root S3 path project root -- point relative consider relative paths. left NULL, root automatically derived cloudfs.s3 field project's DESCRIPTION file.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_s3_write_bulk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write objects to S3 in bulk — cloud_s3_write_bulk","text":"","code":"if (FALSE) { # write two csv files: data/df_mtcars.csv and data/df_iris.csv cloud_object_ls(   dplyr::lst(mtcars = mtcars, iris = iris),   path = \"data\",   extension = \"csv\",   prefix = \"df_\" ) %>%  cloud_s3_write_bulk() }"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate file path for cloud functions — cloud_validate_file_path","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"Makes sure file path passed cloud function right format.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"","code":"cloud_validate_file_path(file, error = TRUE)"},{"path":"https://g6t.github.io/cloudfs/reference/cloud_validate_file_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate file path for cloud functions — cloud_validate_file_path","text":"file Path file relative project folder root. Can contain letters, digits, '-', '_', '.', spaces '/' symbols. error TRUE (default), throws error file valid file path.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/cloudfs-package.html","id":null,"dir":"Reference","previous_headings":"","what":"cloudfs: Streamlined interface to interact with cloud storage platforms — cloudfs-package","title":"cloudfs: Streamlined interface to interact with cloud storage platforms — cloudfs-package","text":"package offers unified interface simplifying cloud storage interactions, including uploading, downloading, reading, writing files, functions Google Drive Amazon S3.","code":""},{"path":[]},{"path":"https://g6t.github.io/cloudfs/reference/cloudfs-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"cloudfs: Streamlined interface to interact with cloud storage platforms — cloudfs-package","text":"Maintainer: Iaroslav Domin iaroslav@gradientmetrics.com Authors: Stefan Musch stefan@gradientmetrics.com Michal Czyz michal@gradientmetrics.com Emmanuel Ugochukwu emmanuel@gradientmetrics.com contributors: Gradient Metrics [copyright holder, funder]","code":""},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from DESCRUPTION file — proj_desc_get","title":"Extract values from DESCRUPTION file — proj_desc_get","text":"Extract values DESCRUPTION file","code":""},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from DESCRUPTION file — proj_desc_get","text":"","code":"proj_desc_get(key, project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/proj_desc_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from DESCRUPTION file — proj_desc_get","text":"key Character. field search DESCRIPTION file. project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate project's DESCRIPTION file — validate_desc","title":"Validate project's DESCRIPTION file — validate_desc","text":"Checks DESCRIPTION file exists project folder. case, proposes create DESCRIPTION file template.","code":""},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate project's DESCRIPTION file — validate_desc","text":"","code":"validate_desc(project = \".\")"},{"path":"https://g6t.github.io/cloudfs/reference/validate_desc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate project's DESCRIPTION file — validate_desc","text":"project Character. Path project. default current working directory.","code":""},{"path":"https://g6t.github.io/cloudfs/news/index.html","id":"cloudfs-001","dir":"Changelog","previous_headings":"","what":"cloudfs 0.0.1","title":"cloudfs 0.0.1","text":"Initial version.","code":""}]
